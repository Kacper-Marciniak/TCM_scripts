{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "import numpy as np\n",
    "import os, sys, json, random\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from PARAMETERS import *\n",
    "\n",
    "\n",
    "from tidecv import TIDE, datasets\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "import json\n",
    "import statistics\n",
    "import glob\n",
    "import os\n",
    "import cv2 as cv\n",
    "\n",
    "# for evaluation without training\n",
    "from PARAMETERS import *\n",
    "from tkinter_dialog_custom import askdirectory\n",
    "\n",
    "while True:\n",
    "    CURRENT_DATASET = askdirectory(title=\"Select dataset folder\", initialdir=PATH_TRAINING_DATA_SEGMENTATION)\n",
    "    if os.path.exists(CURRENT_DATASET): break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "network_config = r\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
    "cfg.merge_from_file(model_zoo.get_config_file(network_config))\n",
    "cfg.OUTPUT_DIR =  askdirectory(initialdir=PATH_MODELS_SEGMENTATION,title=\"Select trained model\")\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.DATASETS.TEST = (\"TCM_test\", )\n",
    "cfg.DATALOADER.NUM_WORKERS = 1\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4\n",
    "cfg.OUTPUT_DIR =  PATH_TRAINING_OUTPUT_DIR_SEGMENTATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_eval_old(segmentation_predictor,path_current_dataset):\n",
    "    pth = os.path.join(path_current_dataset, r'test').replace('\\\\\\\\','\\\\').replace('\\\\','/')\n",
    "    failures_dictionary = DICTIONARY_FAILURES\n",
    "    \n",
    "    F1_score_global = []\n",
    "    Specificity_global = []\n",
    "    Accuracy_global = []\n",
    "    \n",
    "    F1_score_global_stepienie, F1_score_global_narost, F1_score_global_zatarcie, F1_score_global_wykruszenie = [],[],[],[]\n",
    "\n",
    "    for label in glob.glob(f'{pth}/*json'): # iterate through eval dataset\n",
    "        base_name = (label.replace('\\\\\\\\','\\\\').replace('\\\\','/').split('/')[-1]).split('.')[0]\n",
    "        file = open(label, 'r')\n",
    "        json_label = json.load(file )\n",
    "        im = cv.imread(os.path.join(pth,f\"{base_name}.png\"))\n",
    "        print(os.path.join(pth,f\"{base_name}.png\"))\n",
    "        \n",
    "        label_outputs = {\n",
    "            \"wykruszenie\": np.zeros_like(im),\n",
    "            \"narost\": np.zeros_like(im),\n",
    "            \"stepienie\": np.zeros_like(im),\n",
    "            \"zatarcie\": np.zeros_like(im)\n",
    "        }\n",
    "\n",
    "        # Iterate over labels from .json file, merge it into 4 categories, draw it as bitmaps\n",
    "        for shape in json_label[\"shapes\"]:\n",
    "            point_list = np.array(shape['points'])\n",
    "            if len(point_list) > 2:\n",
    "                pts = point_list.reshape((-1, 1, 2))  \n",
    "                for class_name in failures_dictionary.values(): # Iterate over all instances classes\n",
    "                    if(shape[\"label\"]==class_name):\n",
    "                        label_outputs[class_name] = cv.fillPoly(label_outputs[class_name], np.int32([pts]), (255,255,255))\n",
    "                        \n",
    "        # Segmentation model inference\n",
    "        predictions = segmentation_predictor(im) # Make prediction \n",
    "        pred_masks = predictions[\"instances\"].to(\"cpu\").pred_masks.numpy()\n",
    "        pred_classes = predictions[\"instances\"].to(\"cpu\").pred_classes.numpy()\n",
    "        num_instances = pred_masks.shape[0] \n",
    "        pred_masks = np.moveaxis(pred_masks, 0, -1)\n",
    "        pred_masks_instance = []\n",
    "        output = np.zeros_like(im)\n",
    "        \n",
    "        # Contains merged bitmaps for particular failures classes\n",
    "        inference_outputs = {\n",
    "            \"wykruszenie\": np.zeros_like(im),\n",
    "            \"narost\": np.zeros_like(im),\n",
    "            \"stepienie\": np.zeros_like(im),\n",
    "            \"zatarcie\": np.zeros_like(im)\n",
    "        }\n",
    "        # Contains temporary data used during merging\n",
    "        pred_masks_instance = {   \n",
    "            \"wykruszenie\": [],\n",
    "            \"narost\": [],\n",
    "            \"stepienie\": [],\n",
    "            \"zatarcie\": []\n",
    "        }\n",
    "\n",
    "        # Iterate over predicted defects, search for duplicated instances of the same class and merge them into single bitmap.\n",
    "        for i in range(num_instances): \n",
    "            for class_id in failures_dictionary: # Iterate over all instances classes\n",
    "                if(pred_classes[i] == class_id): \n",
    "                    failure_class = failures_dictionary[class_id] # Get name of the current class\n",
    "                    pred_masks_instance[failure_class].append(pred_masks[:, :, i:(i+1)]) \n",
    "                    inference_outputs[failure_class] = np.where(pred_masks_instance[failure_class][-1] == True, 255, inference_outputs[failure_class])\n",
    "    \n",
    "        \n",
    "        F1_score_mean = 0 \n",
    "        F1_counter = 0\n",
    "        F1_score_stepienie_m, F1_score_wykruszenie_m, F1_score_zatarcie_m, F1_score_narost_m = 0,0,0,0\n",
    "\n",
    "        for class_name in failures_dictionary.values():\n",
    "        \n",
    "            pred = cv.cvtColor(inference_outputs[class_name], cv.COLOR_BGR2GRAY)\n",
    "            label = cv.cvtColor(label_outputs[class_name], cv.COLOR_BGR2GRAY)\n",
    "            bitwiseOr = cv.bitwise_or(pred, label)\n",
    "            bitwiseAnd = cv.bitwise_and(pred, label) \n",
    "            bitwiseXor = cv.bitwise_xor(pred, label)\n",
    "            TP = bitwiseAnd  # TP - a sample is predicted to be positive and its label is actually positive\n",
    "            TN = cv.bitwise_not(bitwiseOr) # TN - a sample is predicted to be negative and its label is actually negative\n",
    "            FP = cv.bitwise_xor(TP,pred) # FP - a sample is predicted to be positive and its label is actually negative\n",
    "            FN = cv.bitwise_and(cv.bitwise_not(pred),cv.bitwise_xor(label,TP)) # FN - a sample is predicted to be negative and its label is actually positive\n",
    "\n",
    "            TP = cv.countNonZero(TP)\n",
    "            TN = cv.countNonZero(TN)\n",
    "            FP = cv.countNonZero(FP)\n",
    "            FN = cv.countNonZero(FN)\n",
    "            #acc = (TP+TN)/(TP+TN+FP+FN)\n",
    "            precision = TP/(TP+FP) if (TP+FP) != 0 else 0\n",
    "            recall = TP/(TP+FN) if (TP+FN) != 0 else 0\n",
    "            F1_score = (2*precision*recall)/(precision+recall) if (precision+recall) != 0 else -1\n",
    "\n",
    "            if F1_score != -1:\n",
    "                if class_name == \"stepienie\": F1_score_stepienie_m  += F1_score\n",
    "                if class_name == \"wykruszenie\": F1_score_wykruszenie_m  += F1_score\n",
    "                if class_name == \"narost\": F1_score_narost_m  += F1_score\n",
    "                if class_name == \"zatarcie\": F1_score_zatarcie_m  += F1_score\n",
    "\n",
    "        if 2 in pred_classes: F1_score_global_stepienie.append(F1_score_stepienie_m) \n",
    "        if 1 in pred_classes: F1_score_global_narost.append(F1_score_narost_m)\n",
    "        if 3 in pred_classes: F1_score_global_zatarcie.append(F1_score_zatarcie_m) \n",
    "        if 0 in pred_classes: F1_score_global_wykruszenie.append(F1_score_wykruszenie_m)\n",
    "\n",
    "\n",
    "    F1_score_global = F1_score_global_wykruszenie + F1_score_global_zatarcie + F1_score_global_narost + F1_score_global_stepienie\n",
    "\n",
    "    if len(F1_score_global)==0: F1_score_global.append(-1)\n",
    "    if len(F1_score_global_wykruszenie)==0: F1_score_global_wykruszenie.append(-1)\n",
    "    if len(F1_score_global_zatarcie)==0: F1_score_global_zatarcie.append(-1)\n",
    "    if len(F1_score_global_narost)==0: F1_score_global_narost.append(-1)\n",
    "    if len(F1_score_global_stepienie)==0: F1_score_global_stepienie.append(-1)\n",
    "\n",
    "    print(statistics.mean(F1_score_global), statistics.mean(F1_score_global_stepienie), statistics.mean(F1_score_global_narost), statistics.mean(F1_score_global_zatarcie), statistics.mean(F1_score_global_wykruszenie))\n",
    "    return statistics.mean(F1_score_global), statistics.mean(F1_score_global_stepienie), statistics.mean(F1_score_global_narost), statistics.mean(F1_score_global_zatarcie), statistics.mean(F1_score_global_wykruszenie)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_eval(segmentation_predictor,path_current_dataset):\n",
    "    pth = os.path.join(path_current_dataset, r'test').replace('\\\\\\\\','\\\\').replace('\\\\','/')\n",
    "\n",
    "    metrics = dict()\n",
    "    results = dict()\n",
    "    \n",
    "    for value in list(DICTIONARY_FAILURES.values())+[\"Global\"]:\n",
    "            results[value] = {\n",
    "                \"Recall\":0,\n",
    "                \"Precision\":0,\n",
    "                \"Accuracy\":0,\n",
    "                \"Specificity\":0,\n",
    "                \"F1\":0,\n",
    "            }\n",
    "    \n",
    "    for value in list(DICTIONARY_FAILURES.values())+[\"Global\"]:\n",
    "            metrics[value] = {\n",
    "                \"TP\":[],\n",
    "                \"TN\":[],\n",
    "                \"FP\":[],\n",
    "                \"FN\":[],\n",
    "            }\n",
    "    \n",
    "\n",
    "    for label in glob.glob(f'{pth}/*json'): # iterate through eval dataset\n",
    "        base_name = (label.replace('\\\\\\\\','\\\\').replace('\\\\','/').split('/')[-1]).split('.')[0]\n",
    "        file = open(label, 'r')\n",
    "        json_label = json.load(file )\n",
    "        im = cv.imread(os.path.join(pth,f\"{base_name}.png\"))\n",
    "        pred_classes, image_metrics = calculate_metrics_for_image(im, json_label, segmentation_predictor)\n",
    "        \n",
    "        for class_name in list(image_metrics.keys()):\n",
    "            for metric_name, metric_val in zip(list(image_metrics[class_name].keys()),list(image_metrics[class_name].values())):\n",
    "                metrics[class_name][metric_name].append(metric_val)\n",
    "                metrics[\"Global\"][metric_name].append(metric_val)\n",
    "\n",
    "    # calculate metrics for entire run\n",
    "    for class_name in list(results.keys()):\n",
    "        TP = sum(metrics[class_name][\"TP\"])\n",
    "        TN = sum(metrics[class_name][\"TN\"])\n",
    "        FP = sum(metrics[class_name][\"FP\"])\n",
    "        FN = sum(metrics[class_name][\"FN\"])\n",
    "        accuracy = (TP+TN)/(TP+TN+FP+FN) # Accuracy is a measure of how many of the predictions were correct\n",
    "        precision = TP/(TP+FP) if (TP+FP) != 0 else -1 # Precision is a measure of how many of the positive predictions made are correct (true positives)\n",
    "        recall = TP/(TP+FN) if (TP+FN) != 0 else -1 # Recall is a measure of how many of the positive cases the classifier correctly predicted\n",
    "        specificity = TN / (TN + FP) # Specificity is a measure of how many negative predictions made are correct (true negatives). \n",
    "        F1_score = (2*precision*recall) / (precision+recall) if (precision != -1 and recall != -1) else -1\n",
    "        results[class_name][\"Recall\"] = recall\n",
    "        results[class_name][\"Accuracy\"] = accuracy\n",
    "        results[class_name][\"Precision\"] = precision\n",
    "        results[class_name][\"Specificity\"] = specificity\n",
    "        results[class_name][\"F1\"] = F1_score\n",
    "\n",
    "   \n",
    "    for class_name in list(results.keys()):\n",
    "        print(\n",
    "            f\"\"\"\n",
    "            For class {class_name}\n",
    "                F1 = {results[class_name][\"F1\"]}\n",
    "            \"\"\"\n",
    "        )\n",
    "    return results\n",
    "\n",
    "def calculate_metrics_for_image(im, json_label, segmentation_predictor):\n",
    "        #cv.imshow(\"test\",im)\n",
    "        im_h, im_w, _ = im.shape\n",
    "\n",
    "        label_outputs = dict()\n",
    "        for value in DICTIONARY_FAILURES.values():\n",
    "            label_outputs[str(value)] = np.zeros_like(im)\n",
    "        \n",
    "        # Iterate over labels from .json file, merge it into 4 categories, draw it as bitmaps\n",
    "        for shape in json_label[\"shapes\"]:\n",
    "            point_list = np.array(shape['points'])\n",
    "            if len(point_list) > 2:\n",
    "                pts = point_list.reshape((-1, 1, 2))  \n",
    "                for class_name in DICTIONARY_FAILURES.values(): # Iterate over all instances classes\n",
    "                    if(shape[\"label\"]==class_name):\n",
    "                        label_outputs[class_name] = cv.fillPoly(label_outputs[class_name], np.int32([pts]), (255,255,255))\n",
    "                        \n",
    "        # Segmentation model inference\n",
    "        predictions = segmentation_predictor(im) # Make prediction \n",
    "        pred_masks = predictions[\"instances\"].to(\"cpu\").pred_masks.numpy()\n",
    "        pred_classes = predictions[\"instances\"].to(\"cpu\").pred_classes.numpy()\n",
    "        num_instances = pred_masks.shape[0] \n",
    "        pred_masks = np.moveaxis(pred_masks, 0, -1)\n",
    "        \n",
    "        # Contains merged bitmaps for particular failures classes\n",
    "        inference_outputs = dict()\n",
    "        for value in DICTIONARY_FAILURES.values():\n",
    "            inference_outputs[str(value)] = np.zeros_like(im)\n",
    "\n",
    "        # Contains temporary data used during merging\n",
    "        pred_masks_instance = dict()\n",
    "        for value in DICTIONARY_FAILURES.values():\n",
    "            pred_masks_instance[str(value)] = []\n",
    "\n",
    "        # Iterate over predicted defects, search for duplicated instances of the same class and merge them into single bitmap.\n",
    "        for i in range(num_instances): \n",
    "            for class_id in DICTIONARY_FAILURES: # Iterate over all instances classes\n",
    "                if(pred_classes[i] == class_id): \n",
    "                    failure_class = DICTIONARY_FAILURES[class_id] # Get name of the current class\n",
    "                    pred_masks_instance[failure_class].append(pred_masks[:, :, i:(i+1)]) \n",
    "                    inference_outputs[failure_class] = np.where(pred_masks_instance[failure_class][-1] == True, 255, inference_outputs[failure_class])\n",
    "    \n",
    "        image_metrics = dict()\n",
    "\n",
    "        for value in DICTIONARY_FAILURES.values():\n",
    "            image_metrics[str(value)] = {\n",
    "                \"TP\":0.0,\n",
    "                \"TN\":0.0,\n",
    "                \"FP\":0.0,\n",
    "                \"FN\":0.0,\n",
    "            }\n",
    "\n",
    "        for class_name in DICTIONARY_FAILURES.values():\n",
    "            # iterate through all potential failures\n",
    "        \n",
    "            pred = cv.cvtColor(inference_outputs[class_name], cv.COLOR_BGR2GRAY)\n",
    "            label = cv.cvtColor(label_outputs[class_name], cv.COLOR_BGR2GRAY)\n",
    "            bitwiseOr = cv.bitwise_or(pred, label)\n",
    "            bitwiseAnd = cv.bitwise_and(pred, label) \n",
    "            #bitwiseXor = cv.bitwise_xor(pred, label)\n",
    "\n",
    "            TP = bitwiseAnd  # TP - a sample is predicted to be positive and its label is actually positive\n",
    "            TN = cv.bitwise_not(bitwiseOr) # TN - a sample is predicted to be negative and its label is actually negative\n",
    "            FP = cv.bitwise_xor(TP,pred) # FP - a sample is predicted to be positive and its label is actually negative\n",
    "            FN = cv.bitwise_and(cv.bitwise_not(pred),cv.bitwise_xor(label,TP)) # FN - a sample is predicted to be negative and its label is actually positive\n",
    "\n",
    "            im_size = im_h*im_w\n",
    "\n",
    "            TP = cv.countNonZero(TP) / im_size # normalize to image size\n",
    "            TN = cv.countNonZero(TN) / im_size\n",
    "            FP = cv.countNonZero(FP) / im_size\n",
    "            FN = cv.countNonZero(FN) / im_size\n",
    "\n",
    "            image_metrics[class_name][\"TP\"] = cv.countNonZero(TP)\n",
    "            image_metrics[class_name][\"TN\"] = cv.countNonZero(TN)\n",
    "            image_metrics[class_name][\"FP\"] = cv.countNonZero(FP)\n",
    "            image_metrics[class_name][\"FN\"] = cv.countNonZero(FN)\n",
    "\n",
    "        return pred_classes, image_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_scores = []\n",
    "F1_scores_stepienie, F1_scores_wykruszenie, F1_scores_zatarcie, F1_scores_narost = [],[],[],[]\n",
    "Accuracy, Specificity = [],[]\n",
    "tresholds = []\n",
    "\n",
    "output_metrics = dict()\n",
    "for value in list(DICTIONARY_FAILURES.values())+[\"Global\"]:\n",
    "    output_metrics[value] = {\n",
    "            \"Recall\":[],\n",
    "            \"Precision\":[],\n",
    "            \"Accuracy\":[],\n",
    "            \"Specificity\":[],\n",
    "            \"F1\":[],\n",
    "        }\n",
    "\n",
    "F1_scores_global_old = list()\n",
    "F1_scores_stepienie_old = list()\n",
    "F1_scores_narost_old =  list()\n",
    "F1_scores_zatarcie_old =  list()\n",
    "F1_scores_wykruszenie_old =  list()\n",
    "\n",
    "for TRESHOLD in np.arange(0.3, 1.0, 0.05): #Iterate over various tresholds to find best results\n",
    "    TRESHOLD = float(round(TRESHOLD,2))\n",
    "    tresholds.append(TRESHOLD*100)\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = TRESHOLD\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    test_metadata = MetadataCatalog.get(\"TCM_test\")\n",
    "    print(\"Treshold\",TRESHOLD)\n",
    "    # NEW EVAL\n",
    "    eval_results = custom_eval(predictor, CURRENT_DATASET)\n",
    "    # OLD EVAL\n",
    "    eval_results_old = custom_eval_old(predictor, CURRENT_DATASET)\n",
    "    F1_scores_global_old.append(eval_results_old[0])\n",
    "    F1_scores_stepienie_old.append(eval_results_old[1])\n",
    "    F1_scores_narost_old.append(eval_results_old[2])\n",
    "    F1_scores_zatarcie_old.append(eval_results_old[3])\n",
    "    F1_scores_wykruszenie_old.append(eval_results_old[4])\n",
    "\n",
    "    for class_name in list(eval_results.keys()):\n",
    "        for metric in list(eval_results[class_name].keys()):\n",
    "            output_metrics[class_name][metric].append(eval_results[class_name][metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_folder = PATH_TRAINING_OUTPUT_DIR_SEGMENTATION\n",
    "\n",
    "fig_class_scores, axes = plt.subplots(nrows=2,ncols=2)\n",
    "axes = axes.flatten()\n",
    "\n",
    "fig_class_scores.suptitle(\"Metrics (Classes)\")\n",
    "class_titles = list(DICTIONARY_FAILURES.values())\n",
    "for i,ax in enumerate(axes):\n",
    "    current_class_data = output_metrics[class_titles[i]]\n",
    "    ax.set_xlabel('Treshold [%]')\n",
    "    ax.set_ylabel('Metric')\n",
    "    ax.set_title(class_titles[i])\n",
    "    ax.grid(alpha=0.5)\n",
    "    for key,data in zip(list(current_class_data.keys()),list(current_class_data.values())):\n",
    "        ax.plot(tresholds,data,marker=\"o\",linestyle=\"none\",label=key)\n",
    "    ax.legend()\n",
    "fig_class_scores.set_size_inches(10,10)\n",
    "fig_class_scores.tight_layout()\n",
    "fig_class_scores.savefig(os.path.join(experiment_folder,'F1_class_scores.png'))\n",
    "fig_class_scores.show()\n",
    "\n",
    "fig_global_metrics, axes = plt.subplots(ncols=5)\n",
    "axes = axes.flatten()\n",
    "\n",
    "fig_global_metrics.suptitle(\"Metrics (Global)\")\n",
    "for i,ax in enumerate(axes):\n",
    "    metric_key,metric_val = list(output_metrics[\"Global\"].keys())[i], list(output_metrics[\"Global\"].values())[i]\n",
    "    ax.plot(tresholds,metric_val,marker=\"o\",linestyle=\"none\",label=metric_key)\n",
    "    ax.set_xlabel('Treshold [%]')\n",
    "    ax.set_ylabel('Metric')\n",
    "    ax.grid(alpha=0.5)\n",
    "    ax.set_title(metric_key)\n",
    "fig_global_metrics.set_size_inches(16,6)\n",
    "fig_global_metrics.tight_layout()\n",
    "fig_global_metrics.savefig(os.path.join(experiment_folder,'Global_metrics.png'))\n",
    "fig_global_metrics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_F1_max_old = max(F1_scores_global_old)\n",
    "\n",
    "params = {\n",
    "    \"F1\": global_F1_max_old,\n",
    "    \"tresh\": tresholds[F1_scores_global_old.index(global_F1_max_old)],\n",
    "    \"F1_stepienie\": F1_scores_stepienie_old[F1_scores_global_old.index(global_F1_max_old)],\n",
    "    \"F1_narost\": F1_scores_narost_old[F1_scores_global_old.index(global_F1_max_old)],\n",
    "    \"F1_zatarcie\": F1_scores_zatarcie_old[F1_scores_global_old.index(global_F1_max_old)],\n",
    "    \"F1_wykruszenie\": F1_scores_wykruszenie_old[F1_scores_global_old.index(global_F1_max_old)],\n",
    "}\n",
    "\n",
    "print(\"OLD PARAMETERS\")\n",
    "print(params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('env_main')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58c64561d3c79ee31cfc9210dc9ae71e62823c70a9c0b76afd259aeabe3e4d26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
