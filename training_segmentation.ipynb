{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING SEGMENTATION MODEL\n",
    "Jupyter notebook for training of 2nd stage segmentation model\n",
    "* [Importing modules](#imports)\n",
    "* [Importing training data](#training-data-import)\n",
    "* [Training parameters](#training-parameters)\n",
    "* [Training init](#training-init)\n",
    "* [Neptune parameters tracking](#Neptune-parameters-tracking)\n",
    "* [Training](#training)\n",
    "* [Eval](#Eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "import numpy as np\n",
    "import os, sys, json, random\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from PARAMETERS import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Is CUDA available:',torch.cuda.is_available())\n",
    "print('CUDA version:',torch.version.cuda)\n",
    "print('Torch.version:',torch.__version__)\n",
    "print('Detectron2 version:',detectron2.__version__)\n",
    "print('Device:',torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking allocated memory\n",
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0) \n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "print(f\"{r} {a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_config = r\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
    "base_lr = 0.00025\n",
    "solver_max_iter = 3000\n",
    "eval_period = 100\n",
    "batch_size_per_image = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data.datasets import register_coco_instances\n",
    "from PARAMETERS import PATH_TRAINING_DATA_SEGMENTATION\n",
    "from tkinter_dialog_custom import askdirectory\n",
    "\n",
    "while True:\n",
    "    CURRENT_DATASET = askdirectory(title=\"Select dataset folder\", initialdir=PATH_TRAINING_DATA_SEGMENTATION)\n",
    "    if os.path.exists(CURRENT_DATASET): break\n",
    "\n",
    "\n",
    "# Register a COCO Format Dataset\n",
    "register_coco_instances(\n",
    "    \"TCM_train\",{}, \n",
    "    os.path.join(CURRENT_DATASET,r\"annotations/data_train.json\"),\n",
    "    os.path.join(CURRENT_DATASET,r\"train\"), \n",
    ")\n",
    "register_coco_instances(\n",
    "    \"TCM_val\",{},\n",
    "    os.path.join(CURRENT_DATASET,r\"annotations/data_val.json\"),\n",
    "    os.path.join(CURRENT_DATASET,r\"val\"),\n",
    ")\n",
    "register_coco_instances(\n",
    "    \"TCM_test\",{},\n",
    "    os.path.join(CURRENT_DATASET,r\"annotations/data_test.json\"),\n",
    "    os.path.join(CURRENT_DATASET,r\"test\"), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize training data\n",
    "my_dataset_train_metadata = MetadataCatalog.get(\"TCM_train\")\n",
    "my_dataset_test_metadata = MetadataCatalog.get(\"TCM_test\")\n",
    "my_dataset_val_metadata = MetadataCatalog.get(\"TCM_val\")\n",
    "dataset_dicts = DatasetCatalog.get(\"TCM_test\")\n",
    "\n",
    "images_array = []\n",
    "for d in random.sample(dataset_dicts, 4):\n",
    "    img = cv.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata = my_dataset_train_metadata, scale=0.8,instance_mode = ColorMode.IMAGE_BW)\n",
    "    vis = visualizer.draw_dataset_dict(d)\n",
    "    images_array.append(vis.get_image()[:, :, ::-1])\n",
    "\n",
    "def plotImages(images_arr,nx,ny):\n",
    "    fig, axes = plt.subplots(nx, ny, figsize=(80,nx*20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip(images_arr, axes):\n",
    "        ax.imshow(img)  \n",
    "        ax.axis('Off')\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    \n",
    "plotImages(images_array,1,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neptune init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neptune setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import neptune.new as neptune\n",
    "\n",
    "from PARAMETERS import NEPTUNE_SEGMENTATION_PROJECT_PATH\n",
    "\n",
    "run = neptune.init(\n",
    "    project=NEPTUNE_SEGMENTATION_PROJECT_PATH,\n",
    "    api_token=os.environ.get('NEPTUNE_API_TOKEN'),\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine.hooks import HookBase\n",
    "from detectron2.evaluation import inference_context\n",
    "from detectron2.utils.logger import log_every_n_seconds\n",
    "from detectron2.data import DatasetMapper, build_detection_test_loader\n",
    "import detectron2.utils.comm as comm\n",
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "from detectron2.utils.events import get_event_storage\n",
    "\n",
    "class LossEvalHook(HookBase):\n",
    "    def __init__(self, eval_period, model, data_loader):\n",
    "        self._model = model\n",
    "        self._period = eval_period\n",
    "        self._data_loader = data_loader\n",
    "    \n",
    "    def _do_loss_eval(self):\n",
    "        # Copying inference_on_dataset from evaluator.py\n",
    "        total = len(self._data_loader)\n",
    "        num_warmup = min(5, total - 1)\n",
    "            \n",
    "        start_time = time.perf_counter()\n",
    "        total_compute_time = 0\n",
    "        losses = []\n",
    "        for idx, inputs in enumerate(self._data_loader):            \n",
    "            if idx == num_warmup:\n",
    "                start_time = time.perf_counter()\n",
    "                total_compute_time = 0\n",
    "            start_compute_time = time.perf_counter()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            total_compute_time += time.perf_counter() - start_compute_time\n",
    "            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)\n",
    "            seconds_per_img = total_compute_time / iters_after_start\n",
    "            if idx >= num_warmup * 2 or seconds_per_img > 5:\n",
    "                total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start\n",
    "                eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))\n",
    "                log_every_n_seconds(\n",
    "                    logging.INFO,\n",
    "                    f\"Loss on Validation  done {idx+1:d}/{total:d}. {seconds_per_img:.4f} s / img. ETA={str(eta)}\",\n",
    "                    n=5,\n",
    "                )\n",
    "            loss_batch = self._get_loss(inputs)\n",
    "            losses.append(loss_batch)\n",
    "        mean_loss = np.mean(losses)\n",
    "        self.trainer.storage.put_scalar('validation_loss', mean_loss)\n",
    "        run[\"training/batch/val_loss\"].log(mean_loss)\n",
    "        storage = get_event_storage()\n",
    "        \n",
    "        # AP\n",
    "        segm_AP = \"{:.5g}\".format(storage.history(\"segm/AP\").latest()) \n",
    "        run[\"training/batch/segm_AP\"].log(float(segm_AP))\n",
    "        segm_AP75 = \"{:.5g}\".format(storage.history(\"segm/AP75\").latest()) \n",
    "        run[\"training/batch/segm_AP75\"].log(float(segm_AP75))\n",
    "        \n",
    "        # Classes AP\n",
    "        segm_AP_narost = \"{:.5g}\".format(storage.history(\"segm/AP-narost\").latest()) \n",
    "        run[\"training/batch/segm_AP_narost\"].log(float(segm_AP_narost))\n",
    "        segm_AP_stepienie = \"{:.5g}\".format(storage.history(\"segm/AP-stepienie\").latest()) \n",
    "        run[\"training/batch/segm_AP_stepienie\"].log(float(segm_AP_stepienie))\n",
    "        segm_AP_wykruszenie = \"{:.5g}\".format(storage.history(\"segm/AP-wykruszenie\").latest()) \n",
    "        run[\"training/batch/segm_AP_wykruszenie\"].log(float(segm_AP_wykruszenie))\n",
    "        segm_AP_zatarcie = \"{:.5g}\".format(storage.history(\"segm/AP-zatarcie\").latest()) \n",
    "        run[\"training/batch/segm_AP_zatarcie\"].log(float(segm_AP_zatarcie))    \n",
    "        comm.synchronize()\n",
    "\n",
    "        return losses\n",
    "            \n",
    "    def _get_loss(self, data):\n",
    "        # How loss is calculated on train_loop \n",
    "        metrics_dict = self._model(data)\n",
    "        metrics_dict = {\n",
    "            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\n",
    "            for k, v in metrics_dict.items()\n",
    "        }\n",
    "        total_losses_reduced = sum(loss for loss in metrics_dict.values())\n",
    "        return total_losses_reduced\n",
    "           \n",
    "    def track_metrics(self):\n",
    "        # Getting metrics from the detectron event storage \n",
    "        storage = get_event_storage()\n",
    "        try:    lr = \"{:.5g}\".format(storage.history(\"lr\").latest()) \n",
    "        except  KeyError: lr = \"N/A\"\n",
    "        try:    total_loss = \"{:.5g}\".format(storage.history(\"total_loss\").latest()) \n",
    "        except  KeyError: total_loss = \"N/A\"\n",
    "        try:    loss_cls = \"{:.5g}\".format(storage.history(\"loss_cls\").latest()) \n",
    "        except  KeyError: loss_cls = \"N/A\"\n",
    "        try:    loss_box_reg = \"{:.5g}\".format(storage.history(\"loss_box_reg\").latest()) \n",
    "        except  KeyError: loss_box_reg = \"N/A\"\n",
    "        try:    loss_mask = \"{:.5g}\".format(storage.history(\"loss_mask\").latest()) \n",
    "        except  KeyError: loss_mask = \"N/A\"\n",
    "        try:    loss_rpn_cls = \"{:.5g}\".format(storage.history(\"loss_rpn_cls\").latest()) \n",
    "        except  KeyError: loss_rpn_cls = \"N/A\"  \n",
    "        try:    loss_rpn_loc = \"{:.5g}\".format(storage.history(\"loss_rpn_loc\").latest()) \n",
    "        except  KeyError: loss_rpn_loc = \"N/A\"       \n",
    "\n",
    "        # Saving those metrics to the neptune\n",
    "        run[\"training/batch/lr\"].log(float(lr))\n",
    "        run[\"training/batch/total_loss\"].log(float(total_loss))\n",
    "        run[\"training/batch/loss_cls\"].log(float(loss_cls))\n",
    "        run[\"training/batch/loss_box_reg\"].log(float(loss_box_reg))\n",
    "        run[\"training/batch/loss_mask\"].log(float(loss_mask))\n",
    "        run[\"training/batch/loss_rpn_cls\"].log(float(loss_rpn_cls))\n",
    "        run[\"training/batch/loss_rpn_loc\"].log(float(loss_rpn_loc))\n",
    "\n",
    "             \n",
    "    def after_step(self):\n",
    "        next_iter = self.trainer.iter + 1\n",
    "        is_final = next_iter == self.trainer.max_iter\n",
    "        # Perform loss evaluation every cfg.TEST.EVAL_PERIOD iterations\n",
    "        if is_final or (self._period > 0 and next_iter % self._period == 0):\n",
    "            self._do_loss_eval()\n",
    "        self.trainer.storage.put_scalars(timetest=12)\n",
    "        # Catch metrics every 20 iterations\n",
    "        if self.trainer.iter % 20 == 0:\n",
    "            self.track_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "\n",
    "class MyTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        return COCOEvaluator(dataset_name, cfg, True, output_folder)\n",
    "                     \n",
    "    def build_hooks(self):\n",
    "        hooks = super().build_hooks()\n",
    "        hooks.insert(-1,LossEvalHook(\n",
    "            cfg.TEST.EVAL_PERIOD,\n",
    "            self.model,\n",
    "            build_detection_test_loader(\n",
    "                self.cfg,\n",
    "                self.cfg.DATASETS.TEST[0],\n",
    "                DatasetMapper(self.cfg,True)\n",
    "            )\n",
    "        ))\n",
    "        return hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying traning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from PARAMETERS import PATH_TRAINING_OUTPUT_DIR_SEGMENTATION\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(network_config))\n",
    "cfg.DATASETS.TRAIN = (\"TCM_train\",)\n",
    "cfg.DATASETS.TEST = (\"TCM_val\",)\n",
    "cfg.DATALOADER.NUM_WORKERS = 1\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(network_config)\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = base_lr\n",
    "cfg.SOLVER.MAX_ITER = solver_max_iter\n",
    "cfg.TEST.EVAL_PERIOD = eval_period\n",
    "cfg.SOLVER.STEPS = []\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = batch_size_per_image\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4\n",
    "cfg.OUTPUT_DIR =  PATH_TRAINING_OUTPUT_DIR_SEGMENTATION\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok = True)\n",
    "trainer = MyTrainer(cfg) \n",
    "trainer.resume_or_load(resume = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neptune parameters tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"model\": network_config.replace(\"\\\\\\\\\",\"\\\\\").replace(\"\\\\\",\"/\").split(\"/\")[-1],\n",
    "    \"n_classes\": cfg.MODEL.ROI_HEADS.NUM_CLASSES,\n",
    "    \"lr\": cfg.SOLVER.BASE_LR,\n",
    "    \"max_iter\": cfg.SOLVER.MAX_ITER,\n",
    "    \"eval_period\": cfg.TEST.EVAL_PERIOD,\n",
    "    \"batch_size\": cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE,\n",
    "    \"items_per_batch\": cfg.SOLVER.IMS_PER_BATCH,\n",
    "    \"num_workers\": cfg.DATALOADER.NUM_WORKERS,\n",
    "    \"device\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\",\n",
    "    \"dataset\": CURRENT_DATASET.replace(\"\\\\\",\"/\").split(\"/\")[-1]\n",
    "}\n",
    "run[\"config/parameters\"] = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = {\n",
    "    \"train\": len(DatasetCatalog.get(\"TCM_train\")),\n",
    "    \"val\": len(DatasetCatalog.get(\"TCM_val\")),\n",
    "    \"test\": len(DatasetCatalog.get(\"TCM_test\")) \n",
    "}\n",
    "\n",
    "\n",
    "run[\"config/dataset/dataset_size\"] = dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short summary of the main model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from PARAMETERS import PATH_TRAINING_OUTPUT_DIR_SEGMENTATION\n",
    "\n",
    "experiment_folder = PATH_TRAINING_OUTPUT_DIR_SEGMENTATION\n",
    "\n",
    "def load_json_arr(json_path):\n",
    "    lines = []\n",
    "    with open(json_path, 'r') as f:\n",
    "        for line in f:\n",
    "            lines.append(json.loads(line))\n",
    "    return lines\n",
    "\n",
    "experiment_metrics = load_json_arr(experiment_folder + '/metrics.json')\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Loss')\n",
    "try:\n",
    "    ax1.plot(\n",
    "        [x['iteration'] for x in experiment_metrics if 'total_loss' in x],\n",
    "        [x['total_loss'] for x in experiment_metrics if 'total_loss' in x], color=\"black\", label=\"Total Loss\", linestyle=\"none\", marker=\"o\")\n",
    "except ValueError: pass\n",
    "try:\n",
    "    ax1.plot(\n",
    "        [x['iteration'] for x in experiment_metrics if 'validation_loss' in x], \n",
    "        [x['validation_loss'] for x in experiment_metrics if 'validation_loss' in x], color=\"dimgray\", label=\"Val Loss\", linestyle=\"none\", marker=\"o\")\n",
    "except ValueError: pass\n",
    "ax1.tick_params(axis='y')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "color = 'tab:orange'\n",
    "ax2.set_ylabel('AP')\n",
    "try:\n",
    "    ax2.plot(\n",
    "        [x['iteration'] for x in experiment_metrics if 'validation_loss' in x], \n",
    "        [x['bbox/AP'] for x in experiment_metrics if 'bbox/AP' in x], color=color, label=\"Val AP\", linestyle=\"none\", marker=\"o\")\n",
    "except ValueError: pass\n",
    "ax2.tick_params(axis='y')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(alpha=0.5)\n",
    "fig.set_size_inches(10,8)\n",
    "plt.title(\"Traning parameters\")\n",
    "plt.savefig(os.path.join(experiment_folder,'model_metrics.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tidecv import TIDE, datasets\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "import json\n",
    "import statistics\n",
    "import glob\n",
    "\n",
    "# for evaluation without training\n",
    "from PARAMETERS import *\n",
    "from tkinter_dialog_custom import askdirectory\n",
    "try:\n",
    "    CURRENT_DATASET\n",
    "except NameError:\n",
    "    print(\"No dataset selected! Please select a dataset:\")\n",
    "    while True:\n",
    "        CURRENT_DATASET = askdirectory(title=\"Select dataset folder\", initialdir=PATH_TRAINING_DATA_SEGMENTATION)\n",
    "        if os.path.exists(CURRENT_DATASET): break\n",
    "\n",
    "\n",
    "path_to_tide = os.path.join(PATH_TRAINING_OUTPUT_DIR_SEGMENTATION,r'tide')\n",
    "path_to_coco_dataset = os.path.join(CURRENT_DATASET, r'annotations\\data_test.json')\n",
    "path_to_coco_eval = os.path.join(PATH_TRAINING_OUTPUT_DIR_SEGMENTATION,r'COCO_eval')\n",
    "path_to_coco_results = os.path.join(path_to_coco_eval,r'coco_instances_results.json')\n",
    "\n",
    "# Calculate TIDE metrics based on COCOEvaluator\n",
    "def TIDE_evaluation():\n",
    "    tide = TIDE(pos_threshold=0.75)\n",
    "    gt = datasets.COCO(path_to_coco_dataset)\n",
    "    tide.evaluate(gt, datasets.COCOResult(path_to_coco_results),mode=TIDE.BOX) \n",
    "    tide.plot(f\"{path_to_tide}\\\\tresh_{str(THRESHOLD)}\")       # Show a summary figure. Specify a folder and it'll output a png to that folder.\n",
    "    tide.evaluate(gt, datasets.COCOResult(path_to_coco_results),mode=TIDE.MASK) \n",
    "    tide.summarize()  # Summarize the results as tables in the console\n",
    "    tide.plot(f\"{path_to_tide}\\\\tresh_{str(THRESHOLD)}\")       # Show a summary figure. Specify a folder and it'll output a png to that folder.\n",
    "    \n",
    "    # Passing tide eval results to the Neptune \n",
    "    tide_eval = tide.get_main_errors()['coco_instances_results']\n",
    "    run[\"eval/tide_mask/tresh_\" + str(THRESHOLD)] = tide_eval\n",
    "\n",
    "# Inference from test dataset, provide basic COCO AP metrics\n",
    "def COCO_evaluation():\n",
    "    #Call the COCO Evaluator function and pass the Validation Dataset\n",
    "    evaluator = COCOEvaluator(\"TCM_test\", cfg, False, output_dir = path_to_coco_eval)\n",
    "    val_loader = build_detection_test_loader(cfg, \"TCM_test\")\n",
    "\n",
    "    # Use the created predicted model in the previous step\n",
    "    results = inference_on_dataset(predictor.model, val_loader, evaluator)\n",
    "    \n",
    "    # Passing coco eval results to the Neptune \n",
    "    run[\"eval/coco_segm/tresh_\" + str(THRESHOLD)] = results['segm']\n",
    "    run[\"eval/coco_bbox/tresh_\" + str(THRESHOLD)] = results['bbox']\n",
    "\n",
    "def custom_eval(segmentation_predictor,path_current_dataset):\n",
    "    pth = os.path.join(path_current_dataset, r'test').replace('\\\\\\\\','\\\\').replace('\\\\','/')\n",
    "\n",
    "    metrics = dict()\n",
    "    results = dict()\n",
    "    \n",
    "    for value in list(DICTIONARY_FAILURES.values())+[\"Global\"]:\n",
    "            results[value] = {\n",
    "                \"Recall\":0,\n",
    "                \"Precision\":0,\n",
    "                \"Accuracy\":0,\n",
    "                \"Specificity\":0,\n",
    "                \"F1\":0,\n",
    "            }\n",
    "    \n",
    "    for value in list(DICTIONARY_FAILURES.values())+[\"Global\"]:\n",
    "            metrics[value] = {\n",
    "                \"TP\":[],\n",
    "                \"TN\":[],\n",
    "                \"FP\":[],\n",
    "                \"FN\":[],\n",
    "            }\n",
    "    \n",
    "\n",
    "    for label in glob.glob(f'{pth}/*json'): # iterate through eval dataset\n",
    "        base_name = (label.replace('\\\\\\\\','\\\\').replace('\\\\','/').split('/')[-1]).split('.')[0]\n",
    "        file = open(label, 'r')\n",
    "        json_label = json.load(file )\n",
    "        im = cv.imread(os.path.join(pth,f\"{base_name}.png\"))\n",
    "        pred_classes, image_metrics = calculate_metrics_for_image(im, json_label, segmentation_predictor)\n",
    "        \n",
    "        for class_name in list(image_metrics.keys()):\n",
    "            for metric_name, metric_val in zip(list(image_metrics[class_name].keys()),list(image_metrics[class_name].values())):\n",
    "                metrics[class_name][metric_name].append(metric_val)\n",
    "                metrics[\"Global\"][metric_name].append(metric_val)\n",
    "\n",
    "    # calculate metrics for entire run\n",
    "    for class_name in list(results.keys()):\n",
    "        TP = sum(metrics[class_name][\"TP\"])\n",
    "        TN = sum(metrics[class_name][\"TN\"])\n",
    "        FP = sum(metrics[class_name][\"FP\"])\n",
    "        FN = sum(metrics[class_name][\"FN\"])\n",
    "        accuracy = (TP+TN)/(TP+TN+FP+FN) # Accuracy is a measure of how many of the predictions were correct\n",
    "        precision = TP/(TP+FP) if (TP+FP) != 0 else -1 # Precision is a measure of how many of the positive predictions made are correct (true positives)\n",
    "        recall = TP/(TP+FN) if (TP+FN) != 0 else -1 # Recall is a measure of how many of the positive cases the classifier correctly predicted\n",
    "        specificity = TN / (TN + FP) # Specificity is a measure of how many negative predictions made are correct (true negatives). \n",
    "        F1_score = (2*precision*recall) / (precision+recall) if (precision != -1 and recall != -1) else -1\n",
    "        results[class_name][\"Recall\"] = recall\n",
    "        results[class_name][\"Accuracy\"] = accuracy\n",
    "        results[class_name][\"Precision\"] = precision\n",
    "        results[class_name][\"Specificity\"] = specificity\n",
    "        results[class_name][\"F1\"] = F1_score\n",
    "\n",
    "   \n",
    "    for class_name in list(results.keys()):\n",
    "        print(\n",
    "            f\"\"\"\n",
    "            For class {class_name}\n",
    "                F1 = {results[class_name][\"F1\"]}\n",
    "            \"\"\"\n",
    "        )\n",
    "    return results\n",
    "\n",
    "def calculate_metrics_for_image(im, json_label, segmentation_predictor):\n",
    "        #cv.imshow(\"test\",im)\n",
    "        im_h, im_w, _ = im.shape\n",
    "\n",
    "        label_outputs = dict()\n",
    "        for value in DICTIONARY_FAILURES.values():\n",
    "            label_outputs[str(value)] = np.zeros_like(im)\n",
    "        \n",
    "        # Iterate over labels from .json file, merge it into 4 categories, draw it as bitmaps\n",
    "        for shape in json_label[\"shapes\"]:\n",
    "            point_list = np.array(shape['points'])\n",
    "            if len(point_list) > 2:\n",
    "                pts = point_list.reshape((-1, 1, 2))  \n",
    "                for class_name in DICTIONARY_FAILURES.values(): # Iterate over all instances classes\n",
    "                    if(shape[\"label\"]==class_name):\n",
    "                        label_outputs[class_name] = cv.fillPoly(label_outputs[class_name], np.int32([pts]), (255,255,255))\n",
    "                        \n",
    "        # Segmentation model inference\n",
    "        predictions = segmentation_predictor(im) # Make prediction \n",
    "        pred_masks = predictions[\"instances\"].to(\"cpu\").pred_masks.numpy()\n",
    "        pred_classes = predictions[\"instances\"].to(\"cpu\").pred_classes.numpy()\n",
    "        num_instances = pred_masks.shape[0] \n",
    "        pred_masks = np.moveaxis(pred_masks, 0, -1)\n",
    "        \n",
    "        # Contains merged bitmaps for particular failures classes\n",
    "        inference_outputs = dict()\n",
    "        for value in DICTIONARY_FAILURES.values():\n",
    "            inference_outputs[str(value)] = np.zeros_like(im)\n",
    "\n",
    "        # Contains temporary data used during merging\n",
    "        pred_masks_instance = dict()\n",
    "        for value in DICTIONARY_FAILURES.values():\n",
    "            pred_masks_instance[str(value)] = []\n",
    "\n",
    "        # Iterate over predicted defects, search for duplicated instances of the same class and merge them into single bitmap.\n",
    "        for i in range(num_instances): \n",
    "            for class_id in DICTIONARY_FAILURES: # Iterate over all instances classes\n",
    "                if(pred_classes[i] == class_id): \n",
    "                    failure_class = DICTIONARY_FAILURES[class_id] # Get name of the current class\n",
    "                    pred_masks_instance[failure_class].append(pred_masks[:, :, i:(i+1)]) \n",
    "                    inference_outputs[failure_class] = np.where(pred_masks_instance[failure_class][-1] == True, 255, inference_outputs[failure_class])\n",
    "    \n",
    "        image_metrics = dict()\n",
    "\n",
    "        for value in DICTIONARY_FAILURES.values():\n",
    "            image_metrics[str(value)] = {\n",
    "                \"TP\":0.0,\n",
    "                \"TN\":0.0,\n",
    "                \"FP\":0.0,\n",
    "                \"FN\":0.0,\n",
    "            }\n",
    "\n",
    "        for class_name in DICTIONARY_FAILURES.values():\n",
    "            # iterate through all potential failures\n",
    "        \n",
    "            pred = cv.cvtColor(inference_outputs[class_name], cv.COLOR_BGR2GRAY)\n",
    "            label = cv.cvtColor(label_outputs[class_name], cv.COLOR_BGR2GRAY)\n",
    "            bitwiseOr = cv.bitwise_or(pred, label)\n",
    "            bitwiseAnd = cv.bitwise_and(pred, label) \n",
    "            #bitwiseXor = cv.bitwise_xor(pred, label)\n",
    "\n",
    "            TP = bitwiseAnd  # TP - a sample is predicted to be positive and its label is actually positive\n",
    "            TN = cv.bitwise_not(bitwiseOr) # TN - a sample is predicted to be negative and its label is actually negative\n",
    "            FP = cv.bitwise_xor(TP,pred) # FP - a sample is predicted to be positive and its label is actually negative\n",
    "            FN = cv.bitwise_and(cv.bitwise_not(pred),cv.bitwise_xor(label,TP)) # FN - a sample is predicted to be negative and its label is actually positive\n",
    "\n",
    "            im_size = im_h*im_w\n",
    "\n",
    "            TP = cv.countNonZero(TP) / im_size # normalize to image size\n",
    "            TN = cv.countNonZero(TN) / im_size\n",
    "            FP = cv.countNonZero(FP) / im_size\n",
    "            FN = cv.countNonZero(FN) / im_size\n",
    "\n",
    "            image_metrics[class_name][\"TP\"] = cv.countNonZero(TP)\n",
    "            image_metrics[class_name][\"TN\"] = cv.countNonZero(TN)\n",
    "            image_metrics[class_name][\"FP\"] = cv.countNonZero(FP)\n",
    "            image_metrics[class_name][\"FN\"] = cv.countNonZero(FN)\n",
    "\n",
    "        return pred_classes, image_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_scores = []\n",
    "F1_scores_stepienie, F1_scores_wykruszenie, F1_scores_zatarcie, F1_scores_narost = [],[],[],[]\n",
    "Accuracy, Specificity = [],[]\n",
    "thresholds = []\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.DATASETS.TEST = (\"TCM_test\", )\n",
    "\n",
    "output_metrics = dict()\n",
    "for value in list(DICTIONARY_FAILURES.values())+[\"Global\"]:\n",
    "    output_metrics[value] = {\n",
    "            \"Recall\":[],\n",
    "            \"Precision\":[],\n",
    "            \"Accuracy\":[],\n",
    "            \"Specificity\":[],\n",
    "            \"F1\":[],\n",
    "        }\n",
    "\n",
    "\n",
    "for THRESHOLD in np.arange(0.3, 1.0, 0.05): #Iterate over various thresholds to find best results\n",
    "    THRESHOLD = float(round(THRESHOLD,2))\n",
    "    thresholds.append(THRESHOLD*100)\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = THRESHOLD\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    test_metadata = MetadataCatalog.get(\"TCM_test\")\n",
    "    #COCO_evaluation() # include coco\n",
    "    #TIDE_evaluation() # include tide metrics\n",
    "    print(\"Threshold\",THRESHOLD)\n",
    "    eval_results = custom_eval(predictor, CURRENT_DATASET)\n",
    "    for class_name in list(eval_results.keys()):\n",
    "        for metric in list(eval_results[class_name].keys()):\n",
    "            output_metrics[class_name][metric].append(eval_results[class_name][metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_folder = PATH_TRAINING_OUTPUT_DIR_SEGMENTATION\n",
    "\n",
    "fig_class_scores, axes = plt.subplots(nrows=2,ncols=2)\n",
    "axes = axes.flatten()\n",
    "\n",
    "fig_class_scores.suptitle(\"Metrics (Classes)\")\n",
    "class_titles = list(DICTIONARY_FAILURES.values())\n",
    "for i,ax in enumerate(axes):\n",
    "    current_class_data = output_metrics[class_titles[i]]\n",
    "    ax.set_xlabel('Threshold [%]')\n",
    "    ax.set_ylabel('Metric')\n",
    "    ax.set_title(class_titles[i])\n",
    "    ax.grid(alpha=0.5)\n",
    "    for key,data in zip(list(current_class_data.keys()),list(current_class_data.values())):\n",
    "        ax.plot(thresholds,data,marker=\"o\",linestyle=\"none\",label=key)\n",
    "    ax.legend()\n",
    "fig_class_scores.set_size_inches(10,10)\n",
    "fig_class_scores.tight_layout()\n",
    "fig_class_scores.savefig(os.path.join(experiment_folder,'F1_class_scores.png'))\n",
    "fig_class_scores.show()\n",
    "\n",
    "fig_global_metrics, axes = plt.subplots(ncols=5)\n",
    "axes = axes.flatten()\n",
    "\n",
    "fig_global_metrics.suptitle(\"Metrics (Global)\")\n",
    "for i,ax in enumerate(axes):\n",
    "    metric_key,metric_val = list(output_metrics[\"Global\"].keys())[i], list(output_metrics[\"Global\"].values())[i]\n",
    "    ax.plot(thresholds,metric_val,marker=\"o\",linestyle=\"none\",label=metric_key)\n",
    "    ax.set_xlabel('Threshold [%]')\n",
    "    ax.set_ylabel('Metric')\n",
    "    ax.grid(alpha=0.5)\n",
    "    ax.set_title(metric_key)\n",
    "fig_global_metrics.set_size_inches(16,6)\n",
    "fig_global_metrics.tight_layout()\n",
    "fig_global_metrics.savefig(os.path.join(experiment_folder,'Global_metrics.png'))\n",
    "fig_global_metrics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"params = {\n",
    "    \"F1\": max(F1_scores),\n",
    "    \"tresh\": thresholds[F1_scores.index(max(F1_scores))],\n",
    "    \"F1_stepienie\": F1_scores_stepienie[F1_scores.index(max(F1_scores))],\n",
    "    \"F1_narost\": F1_scores_narost[F1_scores.index(max(F1_scores))],\n",
    "    \"F1_zatarcie\": F1_scores_zatarcie[F1_scores.index(max(F1_scores))],\n",
    "    \"F1_wykruszenie\": F1_scores_wykruszenie[F1_scores.index(max(F1_scores))],\n",
    "    \"accuracy\": Accuracy[F1_scores.index(max(F1_scores))],\n",
    "    \"specificity\": Specificity[F1_scores.index(max(F1_scores))],\n",
    "}\"\"\"\n",
    "params=0\n",
    "try:\n",
    "    run[\"eval/F1_score_best\"] = params\n",
    "    run[\"eval/F1_scores_classes_img\"].upload(os.path.join(experiment_folder,'F1_class_scores.png'))\n",
    "    run[\"eval/Global_metrics_img\"].upload(os.path.join(experiment_folder,'Global_metrics.png'))\n",
    "\n",
    "    from neptune.new.types import File\n",
    "    run[\"eval/F1_scores_classes\"] = File.as_html(fig_class_scores)\n",
    "    run[\"eval/Global_metrics\"] = File.as_html(fig_global_metrics)\n",
    "except NameError:\n",
    "    print(\"Neptune is not initialized\")\n",
    "\n",
    "print(\"Chosen parameters\")\n",
    "for key in params.keys():\n",
    "    print(f\"{key} -> {params[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neptune stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Neptune stop\n",
    "run.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from PIL import Image \n",
    "\n",
    "path_displaying = os.path.join(PATH_TRAINING_OUTPUT_DIR_SEGMENTATION,\"displaying\")\n",
    "path_test_images = os.path.join(CURRENT_DATASET,r'test')\n",
    "os.makedirs(path_displaying, exist_ok = True)\n",
    "\n",
    "def decode_segmentation(imageName):\n",
    "    im = cv.imread(os.path.join(path_test_images,imageName))\n",
    "    outputs = predictor(im)\n",
    "    base_name = imageName.split('.')[0]\n",
    "    pred_masks = outputs[\"instances\"].to(\"cpu\").pred_masks.numpy()\n",
    "    num_instances = pred_masks.shape[0] \n",
    "    pred_masks = np.moveaxis(pred_masks, 0, -1)\n",
    "    pred_masks_instance = []\n",
    "    output = np.zeros_like(im)\n",
    "\n",
    "    scores = outputs[\"instances\"].to(\"cpu\").scores.numpy()\n",
    "    pred_classes = outputs[\"instances\"].to(\"cpu\").pred_classes.numpy()\n",
    "    line = f\"{str(num_instances)}, {str(pred_classes)}, {str(scores)}\"\n",
    "    out_txt_name = (path_displaying,f'{base_name}.txt')\n",
    "    '''\n",
    "    with open(out_txt_name, 'w') as f:\n",
    "        f.write(line)\n",
    "        f.close()\n",
    "    '''\n",
    "    for i in range(num_instances):\n",
    "        out_png_name = path_displaying, f'{base_name}-{i}.png'     \n",
    "        pred_masks_instance.append(pred_masks[:, :, i:(i+1)])\n",
    "        output = np.where(pred_masks_instance[0] == True, 255, output)\n",
    "        im = Image.fromarray(output)\n",
    "        output = np.zeros_like(im)\n",
    "        pred_masks_instance = []\n",
    "        #im.save(out_png_name)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference on random picked images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from detectron2.utils.visualizer import ColorMode\n",
    "import glob\n",
    "pth = r'D:\\Konrad\\TCM_scan\\Skany_nowe_pwr\\pwr_a_1_20210930\\otsu_tooth2'\n",
    "files = os.listdir(pth)\n",
    "\n",
    "for imageName in files:\n",
    "  decode_segmentation(imageName)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from detectron2.utils.visualizer import ColorMode\n",
    "import glob\n",
    "\n",
    "images_array=[]\n",
    "for imageName in glob.glob(f'{CURRENT_DATASET}\\\\test\\\\*png'):\n",
    "  #print(str(imageName))\n",
    "  im = cv.imread(imageName)\n",
    "  outputs = predictor(im)\n",
    "\n",
    "  v = Visualizer(im[:, :, ::-1], metadata=test_metadata,  scale=0.3,instance_mode=ColorMode.SEGMENTATION)\n",
    "  out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "  images_array.append(out.get_image()[:, :, ::-1])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample images plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"plotImages(images_array,4,3)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''path = r'D:\\Konrad\\TCM_scan\\dash_skany\\pwr_a_1_20210930_100324\\otsu_tooth'\n",
    "\n",
    "files = os.listdir(path)\n",
    "containers = []\n",
    "containers.append(('000',0,0))\n",
    "row_max_x = 0\n",
    "row_max_y = 0\n",
    "i = 0\n",
    "prev_name = '000'\n",
    "\n",
    "for image_name in files:   \n",
    "    base_name = image_name[:image_name.rfind('.')]\n",
    "    split_name = base_name.split('_')\n",
    "    row = split_name[1]\n",
    "    im = cv.imread(path + '\\\\' + image_name)\n",
    "\n",
    "    im = cv.cvtColor(im,cv.COLOR_BGR2GRAY)\n",
    "    x,y = im.shape\n",
    "\n",
    "    if (x > row_max_x): \n",
    "        row_max_x = x\n",
    "        containers[i] = (row, row_max_x, row_max_y)\n",
    "    if (y > row_max_y): \n",
    "        row_max_y = y\n",
    "        containers[i] = (row, row_max_x, row_max_y)  \n",
    "    if prev_name != row:\n",
    "        print(containers[i])\n",
    "        prev_name = row\n",
    "        i += 1\n",
    "        row_max_x = 0\n",
    "        row_max_y = 0\n",
    "        containers.append((row, 0, 0))\n",
    "        \n",
    "f.close()'''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "58c64561d3c79ee31cfc9210dc9ae71e62823c70a9c0b76afd259aeabe3e4d26"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('env_main')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
